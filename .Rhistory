Pbi <- B*(2*(Pb0*Pi0))/((Pb0^2)+(2*(Pb0*Pi0)))
Pii <- O
Pab <- AB
(Pa <- ((2*Paa)+Pai+Pab)/(2*N))
(Pb <- ((2*Pbb)+Pbi+Pab)/(2*N))
(Pi <- ((2*Pii)+Pai+Pbi)/(2*N))
counter<-counter+1
}
return(c(paste("Pi =",Pi, ", Pa =", Pa, ", Pb =", Pb, ", Number of loops =", counter)))
}
counter <- 0
EM(Pi,Pa,Pb)
Pi0 <- NULL
Pa0 <- NULL
Pb0 <- NULL
counter <- 0
EM <- function(Pi, Pa, Pb){
while((round(Pi0,12) == round(Pi,12))==FALSE && (round(Pa0,12) == round(Pa,12))==FALSE && (round(Pb0,12) == round(Pb,12))==FALSE)
{
Pi0 <- Pi
Pa0 <- Pa
Pb0 <- Pb
Paa <- A*(Pa0^2/((Pa0^2)+(2*(Pa0*Pi0))))
Pai <- A*(2*(Pa0*Pi0))/((Pa0^2)+(2*(Pa0*Pi0)))
Pbb <- B*(Pb0^2/((Pb0^2)+(2*(Pb0*Pi0))))
Pbi <- B*(2*(Pb0*Pi0))/((Pb0^2)+(2*(Pb0*Pi0)))
Pii <- O
Pab <- AB
(Pa <- ((2*Paa)+Pai+Pab)/(2*N))
(Pb <- ((2*Pbb)+Pbi+Pab)/(2*N))
(Pi <- ((2*Pii)+Pai+Pbi)/(2*N))
counter<-counter+1
}
return(c(paste("Pi =",Pi, ", Pa =", Pa, ", Pb =", Pb, ", Number of loops =", counter)))
}
EM(Pi,Pa,Pb)
Pi0
Pi0 <- NULL
Pa0 <- NULL
Pb0 <- NULL
Pi0
Pa0
Pb0
Pi
round(Pi0,12)
class(Pi0)
round(0,12)
Pi0 <- 0
Pa0 <- 0
Pb0 <- 0
counter <- 0
EM <- function(Pi, Pa, Pb){
while((round(Pi0,12) == round(Pi,12))==FALSE && (round(Pa0,12) == round(Pa,12))==FALSE && (round(Pb0,12) == round(Pb,12))==FALSE)
{
Pi0 <- Pi
Pa0 <- Pa
Pb0 <- Pb
Paa <- A*(Pa0^2/((Pa0^2)+(2*(Pa0*Pi0))))
Pai <- A*(2*(Pa0*Pi0))/((Pa0^2)+(2*(Pa0*Pi0)))
Pbb <- B*(Pb0^2/((Pb0^2)+(2*(Pb0*Pi0))))
Pbi <- B*(2*(Pb0*Pi0))/((Pb0^2)+(2*(Pb0*Pi0)))
Pii <- O
Pab <- AB
(Pa <- ((2*Paa)+Pai+Pab)/(2*N))
(Pb <- ((2*Pbb)+Pbi+Pab)/(2*N))
(Pi <- ((2*Pii)+Pai+Pbi)/(2*N))
counter<-counter+1
}
return(c(paste("Pi =",Pi, ", Pa =", Pa, ", Pb =", Pb, ", Number of loops =", counter)))
}
EM(Pi,Pa,Pb)
EM(Pi,Pa,Pb)
EM(Pi,Pa,Pb)
AB <- 131
A <- 862
B <- 365
O <- 702
(N <- AB + A + B + O)
Pi0 <- 0
Pa0 <- 0
Pb0 <- 0
counter <- 0
EM(Pi,Pa,Pb)
EM <- function(Pi, Pa, Pb){
while((round(Pi0,12) == round(Pi,12))==FALSE && (round(Pa0,12) == round(Pa,12))==FALSE && (round(Pb0,12) == round(Pb,12))==FALSE)
{
Pi0 <- Pi
Pa0 <- Pa
Pb0 <- Pb
Paa <- A*(Pa0^2/((Pa0^2)+(2*(Pa0*Pi0))))
Pai <- A*(2*(Pa0*Pi0))/((Pa0^2)+(2*(Pa0*Pi0)))
Pbb <- B*(Pb0^2/((Pb0^2)+(2*(Pb0*Pi0))))
Pbi <- B*(2*(Pb0*Pi0))/((Pb0^2)+(2*(Pb0*Pi0)))
Pii <- O
Pab <- AB
(Pa <- ((2*Paa)+Pai+Pab)/(2*N))
(Pb <- ((2*Pbb)+Pbi+Pab)/(2*N))
(Pi <- ((2*Pii)+Pai+Pbi)/(2*N))
counter<-counter+1
}
return(c(paste("Pi =",Pi, ", Pa =", Pa, ", Pb =", Pb, ", Number of loops =", counter)))
}
## Specify location of data
#setwd("/Users/katie/Desktop/OceanClimateNovelty/")
setwd("~/Desktop/PostDoc/NovelOceanClim/OceanClimateNovelty/")
source("src/Novelty_Oceans_Functions.R")
#dat <- fread("data/large_files/Data_OceNov.txt", sep = ",")
dat <- fread("data/large_files/ESM2M_2000_RCP8.5.txt", sep = ",")
dat_1800 <- dat %>% filter(Year<1850)
dim(dat_1800)
dat_2000 <-   dat %>% filter(Year>1960 & Year<2010)
dim(dat_2000)
dat_2100 <-   dat %>% filter(Year>2050)
dim(dat_2100)
norm_1800 <- calculate_normals(dat_1800)
norm_2000 <- calculate_normals(dat_2000)
norm_2100 <- calculate_normals(dat_2100)
head(norm_1800)
head(norm_2000)
head(norm_2100)
#--------------------------------
### data frame to link station number to Lat Long ####
#--------------------------------
head(dat)
min(which(dat$No==2))
stations <- unique(dat$No)
stationInfo = data.frame(stations=stations, lat=NA, long=NA)
unik <- which(!duplicated(dat$No))
head(unik)
stationInfo$lat <- dat$Lat[unik]
stationInfo$long <- dat$Lon[unik]
head(stationInfo)
which(!complete.cases(stationInfo))
#--------------------------------
### 1800 analog to today ####
#--------------------------------
length(norm_1800$No)
length(norm_2000$No)
A <- norm_1800
# 1800-1830 climate normals
head(A)
dim(A)
B <- norm_2000[norm_2000$No %in% norm_1800$No,]
# 1970-2000 climate normals
head(B)
dim(B)
# sanity check to make sure stations in right order
identical(A$No, B$No) # should be true
head(dat_2000)
C <- data.frame(dat_2000[,c(1,6,7,8)], dat_2000[,c(6,7,8)])
head(C)
C.id <- C$No
proxy <- B$No
# Principal component truncation rule
trunc.SDs <- 0.1 #truncation
#initiate the data frame to store the projected sigma dissimilarity of best analogs for each grid cell.
NN.sigma <- rep(NA,length(proxy))
for(j in sort(unique(proxy))){
# run the novelty calculation once for each ICV proxy.
# Takes about 1.5 sec/iteration on a typical laptop.
## Select data relevant to ICV proxy j
Bj <- B[which(proxy==j),]   # future climates
# select locations for which ICV proxy j is the closest ICV proxy.
Cj <- C[which(C.id==j),]    # reference period ICV at ICV proxy j
## Step 1: express climate data as standardized anomalies of reference period
#  ICV at ICV proxy j.
Cj.sd <- apply(Cj,2,sd, na.rm=T)  #standard deviation of interannual variability in each climate variable, ignoring missing years
#standard deviation of variability in each climate
# variable, ignoring missing years
A.prime <- sweep(A[,2:7],MARGIN=2,Cj.sd[2:7],`/`) #standardize the reference ICV
# a <- matrix(c(1,2,3,4,5,6), nrow=2)
# sweep(a, MARGIN =2, STATS=c(2,3,4)) # subtracts STATs from each column
# sweep(a, MARGIN =2, STATS=c(2,3,4), FUN=`/`) # divides each column by STATS
Bj.prime <- sweep(Bj[,2:7],MARGIN=2,Cj.sd[2:7],`/`) #standardize the analog pool
Cj.prime <- sweep(Cj[,2:7],MARGIN=2,Cj.sd[2:7],`/`) #standardize the projected future conditions of grid cells represented by ICV proxy j
colnames(Cj.prime) <- colnames(A.prime)
## Step 2: Extract the principal components (PCs) of the reference period ICV
# and project all data onto these PCs
PCA <- prcomp(Cj.prime[!is.na(apply(Cj.prime,1,mean)),])
# Principal components analysis. The !is.na(apply(...)) term is there
# simply to select all years with complete observations in all variables.
PCA$rotation
#plot(PCA$rotation[,1], PCA$rotation[,2], xlim=c(-0.43, -0.39))
#text(PCA$rotation[,1], PCA$rotation[,2], rownames(PCA$rotation))
# SST summer and winter in lower right of PC space,
# Arag and Calc in upper left of PC space
#plot(PCA$rotation[,1], PCA$rotation[,3], xlim=c(-0.43, -0.39))
#text(PCA$rotation[,1], PCA$rotation[,3], rownames(PCA$rotation))
# separates the three variables
#plot(PCA$sdev)
#round(PCA$sdev, 2)
PCs <- max(which(unlist(summary(PCA)[1])>trunc.SDs))
# find the number of PCs to retain using the PC truncation
# rule of eigenvector stdev > the truncation threshold
X <- as.data.frame(predict(PCA,A.prime))
# project the analog pool onto the PCs
head(X)
Yj <- as.data.frame(predict(PCA,Bj.prime))
# project the projected future conditions onto the PCs
Zj <- as.data.frame(predict(PCA,Cj.prime))
# project the reference ICV onto the PCs
#plot(X[,1], X[,2]) # analog
#points(Zj[,1], Zj[,2], pch=19, col=rgb(1,0,0, 0.5)) # reference ICV
#points(Yj[,1], Yj[,2], pch=19, col=rgb(0,1,0)) # future conditions
## Step 3a: express PC scores as standardized anomalies of reference interannual variability
Zj.sd <- apply(Zj,2,sd, na.rm=T)
#standard deviation of 1951-1990 interannual variability in each principal component, ignoring missing years
#Zj.sd
X.prime <- sweep(X,MARGIN=2,Zj.sd,`/`)
#standardize the analog pool
#head(X.prime)
Yj.prime <- sweep(Yj,MARGIN=2,Zj.sd,`/`)
#standardize the projected conditions
#Yj.prime
## Step 3b: find the sigma dissimilarity of each projected condition with
# its best analog (Euclidean nearest neighbour) in the observed analog pool.
X.prime <- X.prime[complete.cases(X.prime),]
NN.dist <- as.vector(get.knnx(data=X.prime[,1:PCs],
query=Yj.prime[,1:PCs],
k=1,algorithm="brute")[[2]])
# Euclidean nearest neighbour distance in the z-standardized PCs of
# interannual climatic variability, i.e. the Mahalanobian nearest neighbour.
NN.chi <- pchi(NN.dist,PCs) # percentile of the nearest neighbour
# distance on the chi distribution with degrees of freedom
# equaling the dimensionality of the distance measurement (PCs)
NN.sigma[which(proxy==j)] <- qchi(NN.chi,1)
# values of the chi percentiles on a standard half-normal distribution (chi distribution with one degree of freedom)
if(j%%10==0){print(j)}
}
dim(A)
dim(B)
head(NN.sigma)
NN.sigma[which(is.infinite(NN.sigma))] <- NA
#which(is.na(NN.sigma))
tail(sort(NN.sigma))
length(NN.sigma)
B2 <- data.frame(No=B$No, NN.sigma)
B2 <- merge(B2, stationInfo, by.x="No", by.y="stations", all.x=TRUE)
getwd()
#write.csv(NN.sigma,"NN.sigma.RCP45.GlobalMean.2085.csv", row.names=FALSE)
write.csv(NN.sigma,"NN.sigma.RCP85.today_1800.csv", row.names=FALSE)
head(NN.sigma)
?write.csv
#write.csv(NN.sigma,"NN.sigma.RCP45.GlobalMean.2085.csv", row.names=FALSE)
write.csv(B2,"Sigma.RCP85.today_1800.csv", row.names=FALSE)
Plot_nonInt(B2$lat, B2$long,
B2$NN.sigma, world, "sigma dis.")
B2 <- data.frame(No=B$No, NN.sigma)
B2 <- merge(B2, stationInfo, by.x="No", by.y="stations", all.x=TRUE)
# Visualize
world <- map_data("world2")
dim(stationInfo)
Plot_nonInt(B2$lat, B2$long,
B2$NN.sigma, world, "sigma dis.")
##Interpolation for visualization
B2a<-B2[!is.na(B2$NN.sigma),]
B2a<-B2a[,c(4,3,2)]
for(i in 1:nrow(B2a)){
if(B2a$long[i]>360){
B2a$long[i]<-B2a$long[i]-360
}
}
EB2 <- SpatialPoints(B2a) # this is your spatial points df
# Project sp object to WGS 84
proj4string(EB2) <- CRS("+proj=longlat +ellps=WGS84 +datum=WGS84")
gr <- as.data.frame(spsample(EB2, 'regular', n  = 50000))
names(gr) <- c('X', 'Y')
coordinates(gr) <- c('X', 'Y')
gridded(gr) <- TRUE
fullgrid(gr) <- TRUE  # Create SpatialGrid object
proj4string(gr) <- proj4string(EB2)
#If this line throws an error, run this
?idw
# Interpolate the grid cells using inverse distance weighing power = 8
# NN.sigma ~ 1 = simple kriging
EB2.idw <- idw(NN.sigma ~ 1, EB2, newdata = gr, idp = 8)
# Convert to raster
r <- raster(EB2.idw)
world<-map("world2", fill=T,plot=F)
y<-map2SpatialPolygons(world, IDs = sapply(strsplit(world$names, ":"), function(x) x[1]), proj4string=CRS("+proj=longlat +datum=WGS84"))
z<-st_as_sf(y)
wr <- raster(z, res = 0.01)
wrld_r <- fasterize(z, wr)
gplot_wrld_r <- gplot_data(wrld_r)
gplot_r <- gplot_data(r)
#Change scale_fill_gradient value to name of variable
ggplot() +
geom_tile(data = gplot_r,
aes(x = x, y = y, fill = value)) +
geom_tile(data = dplyr::filter(gplot_wrld_r, !is.na(value)),
aes(x = x, y = y), fill = "grey20") +
ylim(-78,90) +
xlab("Long") +
ylab("Lat") +
ggtitle("Sigma dissimilarity: Today from 1800") +
theme(plot.title = element_text(hjust = 0.5)) +
scale_fill_gradient2(expression(paste(sigma," dis.")),
low = 'blue', mid = "yellow", high = 'red',
midpoint = 4,
na.value = NA) +
coord_quickmap()
#--------------------------------
### Today analog to 2100 ####
#--------------------------------
identical(norm_2000$No, norm_2100$No)
A <- norm_2000
# 1970-2000 climate normals
head(A)
dim(A)
B <- norm_2100
# 2070-2100
head(B)
dim(B)
# sanity check to make sure stations in right order
identical(A$No, B$No) # should be true
head(dat_2000)
C <- data.frame(dat_2000[,c(1,6,7,8)], dat_2000[,c(6,7,8)])
head(C)
C.id <- C$No
proxy <- B$No
# Principal component truncation rule
trunc.SDs <- 0.1 #truncation
#initiate the data frame to store the projected sigma dissimilarity of best analogs for each grid cell.
NN.sigma <- rep(NA,length(proxy))
for(j in sort(unique(proxy))){
# run the novelty calculation once for each ICV proxy.
# Takes about 1.5 sec/iteration on a typical laptop.
## Select data relevant to ICV proxy j
Bj <- B[which(proxy==j),]   # future climates
# select locations for which ICV proxy j is the closest ICV proxy.
Cj <- C[which(C.id==j),]    # reference period ICV at ICV proxy j
## Step 1: express climate data as standardized anomalies of reference period
#  ICV at ICV proxy j.
Cj.sd <- apply(Cj,2,sd, na.rm=T)  #standard deviation of interannual variability in each climate variable, ignoring missing years
#standard deviation of variability in each climate
# variable, ignoring missing years
A.prime <- sweep(A[,2:7],MARGIN=2,Cj.sd[2:7],`/`) #standardize the reference ICV
# a <- matrix(c(1,2,3,4,5,6), nrow=2)
# sweep(a, MARGIN =2, STATS=c(2,3,4)) # subtracts STATs from each column
# sweep(a, MARGIN =2, STATS=c(2,3,4), FUN=`/`) # divides each column by STATS
Bj.prime <- sweep(Bj[,2:7],MARGIN=2,Cj.sd[2:7],`/`) #standardize the analog pool
Cj.prime <- sweep(Cj[,2:7],MARGIN=2,Cj.sd[2:7],`/`) #standardize the projected future conditions of grid cells represented by ICV proxy j
colnames(Cj.prime) <- colnames(A.prime)
## Step 2: Extract the principal components (PCs) of the reference period ICV
# and project all data onto these PCs
PCA <- prcomp(Cj.prime[!is.na(apply(Cj.prime,1,mean)),])
# Principal components analysis. The !is.na(apply(...)) term is there
# simply to select all years with complete observations in all variables.
PCA$rotation
#plot(PCA$rotation[,1], PCA$rotation[,2], xlim=c(-0.43, -0.39))
#text(PCA$rotation[,1], PCA$rotation[,2], rownames(PCA$rotation))
# SST summer and winter in lower right of PC space,
# Arag and Calc in upper left of PC space
#plot(PCA$rotation[,1], PCA$rotation[,3], xlim=c(-0.43, -0.39))
#text(PCA$rotation[,1], PCA$rotation[,3], rownames(PCA$rotation))
# separates the three variables
#plot(PCA$sdev)
#round(PCA$sdev, 2)
PCs <- max(which(unlist(summary(PCA)[1])>trunc.SDs))
# find the number of PCs to retain using the PC truncation
# rule of eigenvector stdev > the truncation threshold
X <- as.data.frame(predict(PCA,A.prime))
# project the analog pool onto the PCs
head(X)
Yj <- as.data.frame(predict(PCA,Bj.prime))
# project the projected future conditions onto the PCs
Zj <- as.data.frame(predict(PCA,Cj.prime))
# project the reference ICV onto the PCs
#plot(X[,1], X[,2]) # analog
#points(Zj[,1], Zj[,2], pch=19, col=rgb(1,0,0, 0.5)) # reference ICV
#points(Yj[,1], Yj[,2], pch=19, col=rgb(0,1,0)) # future conditions
## Step 3a: express PC scores as standardized anomalies of reference interannual variability
Zj.sd <- apply(Zj,2,sd, na.rm=T)
#standard deviation of 1951-1990 interannual variability in each principal component, ignoring missing years
#Zj.sd
X.prime <- sweep(X,MARGIN=2,Zj.sd,`/`)
#standardize the analog pool
#head(X.prime)
Yj.prime <- sweep(Yj,MARGIN=2,Zj.sd,`/`)
#standardize the projected conditions
#Yj.prime
## Step 3b: find the sigma dissimilarity of each projected condition with
# its best analog (Euclidean nearest neighbour) in the observed analog pool.
X.prime <- X.prime[complete.cases(X.prime),]
NN.dist <- as.vector(get.knnx(data=X.prime[,1:PCs],
query=Yj.prime[,1:PCs],
k=1,algorithm="brute")[[2]])
# Euclidean nearest neighbour distance in the z-standardized PCs of
# interannual climatic variability, i.e. the Mahalanobian nearest neighbour.
NN.chi <- pchi(NN.dist,PCs) # percentile of the nearest neighbour
# distance on the chi distribution with degrees of freedom
# equaling the dimensionality of the distance measurement (PCs)
NN.sigma[which(proxy==j)] <- qchi(NN.chi,1)
# values of the chi percentiles on a standard half-normal distribution (chi distribution with one degree of freedom)
if(j%%10==0){print(j)}
}
dim(A)
dim(B)
head(NN.sigma)
NN.sigma[which(is.infinite(NN.sigma))] <- NA
#which(is.na(NN.sigma))
tail(sort(NN.sigma))
length(NN.sigma)
B2 <- data.frame(No=B$No, NN.sigma)
B2 <- merge(B2, stationInfo, by.x="No", by.y="stations", all.x=TRUE)
dim(A)
dim(B)
head(NN.sigma)
which(is.infinite(NN.sigma))
NN.sigma[which(is.infinite(NN.sigma))] <- NA
which(is.na(NN.sigma))
tail(sort(NN.sigma))
hist(NN.sigma)
length(NN.sigma)
B2 <- data.frame(No=B$No, NN.sigma)
B2 <- merge(B2, stationInfo, by.x="No", by.y="stations", all.x=TRUE)
world <- map_data("world2")
Plot_nonInt(B2$lat, B2$long,
B2$NN.sigma, world, "sigma dis.")
#write.csv(NN.sigma,"NN.sigma.RCP45.GlobalMean.2085.csv", row.names=FALSE)
write.csv(B2,"Sigma.RCP85.today_2100.csv", row.names=FALSE)
#Visualize with interpolation
B2a<-B2[!is.na(B2$NN.sigma),]
B2a<-B2a[,c(4,3,2)]
for(i in 1:nrow(B2a)){
if(B2a$long[i]>360){
B2a$long[i]<-B2a$long[i]-360
}
}
EB2 <- SpatialPoints(B2a) # this is your spatial points df
# Project sp object to WGS 84
proj4string(EB2) <- CRS("+proj=longlat +ellps=WGS84 +datum=WGS84")
gr <- as.data.frame(spsample(EB2, 'regular', n  = 50000))
names(gr) <- c('X', 'Y')
coordinates(gr) <- c('X', 'Y')
gridded(gr) <- TRUE
fullgrid(gr) <- TRUE  # Create SpatialGrid object
proj4string(gr) <- proj4string(EB2)
gr <- as.data.frame(spsample(EB2, 'regular', n  = 50000))
names(gr) <- c('X', 'Y')
coordinates(gr) <- c('X', 'Y')
gridded(gr) <- TRUE
fullgrid(gr) <- TRUE  # Create SpatialGrid object
proj4string(gr) <- proj4string(EB2)
gr <- as.data.frame(spsample(EB2, 'regular', n  = 50000))
names(gr) <- c('X', 'Y')
coordinates(gr) <- c('X', 'Y')
gridded(gr) <- TRUE
fullgrid(gr) <- TRUE  # Create SpatialGrid object
proj4string(gr) <- proj4string(EB2)
EB2.idw <- idw(NN.sigma ~ 1, EB2, newdata = gr, idp = 8)
# Convert to raster
r <- raster(EB2.idw)
world<-map("world2", fill=T,plot=F)
y<-map2SpatialPolygons(world, IDs = sapply(strsplit(world$names, ":"), function(x) x[1]), proj4string=CRS("+proj=longlat +datum=WGS84"))
z<-st_as_sf(y)
wr <- raster(z, res = 0.01)
wrld_r <- fasterize(z, wr)
gplot_wrld_r <- gplot_data(wrld_r)
gplot_r <- gplot_data(r)
#Change scale_fill_gradient value to name of variable
ggplot() +
geom_tile(data = gplot_r,
aes(x = x, y = y, fill = value)) +
geom_tile(data = dplyr::filter(gplot_wrld_r, !is.na(value)),
aes(x = x, y = y), fill = "grey20") +
ylim(-78,90) +
xlab("Long") +
ylab("Lat") +
ggtitle("Sigma dissimilarity: Today from 2100") +
theme(plot.title = element_text(hjust = 0.5)) +
scale_fill_gradient2(expression(paste(sigma," dis.")),
low = 'blue', mid = "yellow", high = 'red',
midpoint = 4,
na.value = NA) +
coord_quickmap()
r
#Change scale_fill_gradient value to name of variable
ggplot() +
geom_tile(data = gplot_r,
aes(x = x, y = y, fill = value)) +
geom_tile(data = dplyr::filter(gplot_wrld_r, !is.na(value)),
aes(x = x, y = y), fill = "grey20") +
xlim(1,359) +
ylim(-77,90) +
xlab("Long") +
ylab("Lat") +
ggtitle("Sigma dissimilarity: Today from 2100") +
theme(plot.title = element_text(hjust = 0.5)) +
scale_fill_gradient2(expression(paste(sigma," dis.")),
low = 'blue', mid = "yellow", high = 'red',
midpoint = 4,
na.value = NA) +
coord_quickmap()
#Change scale_fill_gradient value to name of variable
ggplot() +
geom_tile(data = gplot_r,
aes(x = x, y = y, fill = value)) +
geom_tile(data = dplyr::filter(gplot_wrld_r, !is.na(value)),
aes(x = x, y = y), fill = "grey20") +
xlim(1,358) +
ylim(-77,90) +
xlab("Long") +
ylab("Lat") +
ggtitle("Sigma dissimilarity: Today from 2100") +
theme(plot.title = element_text(hjust = 0.5)) +
scale_fill_gradient2(expression(paste(sigma," dis.")),
low = 'blue', mid = "yellow", high = 'red',
midpoint = 4,
na.value = NA) +
coord_quickmap()
rm(list=ls())
